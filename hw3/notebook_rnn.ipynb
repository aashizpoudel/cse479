{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6defa85-27c4-4b04-887c-42446c3146ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "\n",
    "import tensorflow_datasets as tfds \n",
    "DATA_DIR = \"./tensorflow-datasets\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14ae7732-c72c-4328-a5da-1219437a6ed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-10 08:21:59.347840: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-10 08:22:02.141529: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30972 MB memory:  -> device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:d8:00.0, compute capability: 7.0\n"
     ]
    }
   ],
   "source": [
    "for_vocabulary = tfds.load(\"imdb_reviews\", split=\"train\", with_info=False, as_supervised=True, data_dir=DATA_DIR)\n",
    "train_ds,val_ds = tfds.load('imdb_reviews', split=[\"train[:70%]\",\"train[70%:]\"], with_info=False,\n",
    "                          as_supervised=True,data_dir=DATA_DIR)\n",
    "\n",
    "# train_dataset, test_dataset = dataset['train'], dataset['test']\n",
    "\n",
    "# train_dataset.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed801193-c0f4-4fb4-8a94-f32c5c870873",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17500, 7500, 25000)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_ds), len(val_ds), len(for_vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8931eba-c988-4764-81ac-e33195481a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BATCH_SIZE=64\n",
    "# BUFFER_SIZE=10000\n",
    "# train_batched = train_ds.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "# val_batched = val_ds.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d79f61-4438-4484-b372-733e2804ecd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "244a7936-2cbb-4a2a-8008-d007a4e92347",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-10 08:22:02.615277: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b\"This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie's ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name. I could barely sit through it.\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-10 08:22:02.831114: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "for example, label in train_ds.take(1):\n",
    "    print(example.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "076981b4-01c9-4658-b59b-da1bd92008e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# length = for_vocabulary.map(lambda text,label: len(tf.strings.split(text,\" \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b332dea4-6592-4cf4-9e83-33a185c759c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "def custom_standardization(input_data):\n",
    "    lowercase = tf.strings.lower(input_data)\n",
    "    stripped_html = tf.strings.regex_replace(lowercase, '<br />', ' ')\n",
    "    return tf.strings.regex_replace(stripped_html,\n",
    "                                  '[%s]' % re.escape(string.punctuation),\n",
    "                                  '')\n",
    "\n",
    "def get_text_vectorizer(dataset, vocab_size=1000, sequence_length=500):\n",
    "    vectorizer = tf.keras.layers.TextVectorization(standardize=custom_standardization, max_tokens=vocab_size,output_mode='int',output_sequence_length=sequence_length)\n",
    "    vectorizer.adapt(dataset.map(lambda text,label: text))\n",
    "    return vectorizer \n",
    "\n",
    "def train_model(model, train_ds, val_ds, loss_fn, batch_size=64, optimizer=tf.keras.optimizers.Adam(), callbacks=None, epochs=50):\n",
    "    train_batched = train_ds.shuffle(1000).batch(batch_size)\n",
    "    val_batched = val_ds.shuffle(1000).batch(batch_size)\n",
    "    model.compile(loss=loss_fn, optimizer=optimizer,\n",
    "              metrics=[tf.metrics.BinaryAccuracy(threshold=0.5)])\n",
    "    history = model.fit(train_batched, epochs=epochs, validation_data=val_batched,callbacks=callbacks)\n",
    "    return history\n",
    "\n",
    "from sklearn.metrics import accuracy_score, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt \n",
    "def evaluate_model(model, val_ds, result_path=None):\n",
    "    val_batched = val_ds.batch(64)\n",
    "    predictions = (tf.nn.sigmoid(model.predict(val_batched)) > 0.5).numpy()\n",
    "    truth = np.concatenate([label.numpy() for _,label in val_batched],axis=0)\n",
    "    accuracy = accuracy_score(truth, predictions)\n",
    "    ConfusionMatrixDisplay.from_predictions(y_pred=predictions, y_true=truth,normalize=\"true\")\n",
    "    if result_path:\n",
    "        plt.savefig(result_path.joinpath('confusion_matrix.jpg'))\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()\n",
    "    return accuracy \n",
    "    \n",
    "def plot_training_graphs(history, result_path=None):\n",
    "    plt.plot(history.history['loss'],label=\"training\")\n",
    "    plt.plot(history.history['val_loss'],label=\"validation\")\n",
    "    plt.xlabel(\"epochs\")\n",
    "    if result_path:\n",
    "        plt.savefig(result_path.joinpath('training_history.jpg'))\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "778db6c0-5c41-4ece-9973-0246de0c2d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add attention layer to the deep learning network\n",
    "import keras.backend as K\n",
    "class Attention(tf.keras.layers.Layer):\n",
    "    def __init__(self,**kwargs):\n",
    "        super(Attention,self).__init__(**kwargs)\n",
    " \n",
    "    def build(self,input_shape):\n",
    "        self.W=self.add_weight(name='attention_weight', shape=(input_shape[-1],1), \n",
    "                               initializer='random_normal', trainable=True)\n",
    "        self.b=self.add_weight(name='attention_bias', shape=(input_shape[1],1), \n",
    "                               initializer='zeros', trainable=True)        \n",
    "        super(Attention, self).build(input_shape)\n",
    " \n",
    "    def call(self,x):\n",
    "        # Alignment scores. Pass them through tanh function\n",
    "        e = K.tanh(K.dot(x,self.W)+self.b)\n",
    "        # Remove dimension of size 1\n",
    "        e = K.squeeze(e, axis=-1)   \n",
    "        # Compute the weights\n",
    "        alpha = K.softmax(e)\n",
    "        # Reshape to tensorFlow format\n",
    "        alpha = K.expand_dims(alpha, axis=-1)\n",
    "        # Compute the context vector\n",
    "        context = x * alpha\n",
    "        context =K.sum(context, axis=1)\n",
    "        return context\n",
    "    \n",
    "    \n",
    "def get_bidirectional_lstm_attention(vectorizer,kernel_regularizer=None, use_dropout=False):\n",
    "    input = tf.keras.layers.Input(shape=[None],dtype=tf.string)\n",
    "    vocab_len = len(vectorizer.get_vocabulary())\n",
    "    x = vectorizer(input)\n",
    "    x = tf.keras.layers.Embedding(input_dim=vocab_len, output_dim=64, mask_zero=True,name=\"embedding\")(x)\n",
    "    x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True,kernel_regularizer=kernel_regularizer), name=\"bi_lstm_0\")(x)\n",
    "    # x, forward_h, forward_c, backward_h, backward_c = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32,return_sequences=True, return_state=True), name=\"bi_lstm_0\")(x)\n",
    "    # x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True,kernel_regularizer=kernel_regularizer), name=\"bi_lstm_1\")(x)\n",
    "    x = Attention()(x)\n",
    "    # x = tf.keras.layers.Dense(64, activation='relu',kernel_regularizer=kernel_regularizer )(x)\n",
    "    x = tf.keras.layers.Dense(64, activation='relu', kernel_regularizer=kernel_regularizer)(x)\n",
    "    if use_dropout:\n",
    "        x = tf.keras.layers.Dropout(rate=0.2)(x)\n",
    "    x = tf.keras.layers.Dense(1)(x)\n",
    "    return tf.keras.Model(inputs=input,outputs=x)\n",
    "\n",
    "\n",
    "def get_bidirectional_gru_attention(vectorizer,kernel_regularizer=None, use_dropout=False):\n",
    "    input = tf.keras.layers.Input(shape=[None],dtype=tf.string)\n",
    "    vocab_len = len(vectorizer.get_vocabulary())\n",
    "    x = vectorizer(input)\n",
    "    x = tf.keras.layers.Embedding(input_dim=vocab_len, output_dim=64, mask_zero=True,name=\"embedding\")(x)\n",
    "    x = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(64, return_sequences=True,kernel_regularizer=kernel_regularizer), name=\"bi_gru_0\")(x)\n",
    "    # x = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(64, return_sequences=True,kernel_regularizer=kernel_regularizer), name=\"bi_lstm_0\")(x)\n",
    "    # x, forward_h, forward_c, backward_h, backward_c = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32,return_sequences=True, return_state=True), name=\"bi_lstm_0\")(x)\n",
    "    \n",
    "    x = Attention()(x)\n",
    "    \n",
    "    # x = tf.keras.layers.Dense(64, activation='relu',kernel_regularizer=kernel_regularizer )(x)\n",
    "    x = tf.keras.layers.Dense(64, activation='relu', kernel_regularizer=kernel_regularizer)(x)\n",
    "    if use_dropout:\n",
    "        x = tf.keras.layers.Dropout(rate=0.2)(x)\n",
    "    x = tf.keras.layers.Dense(1)(x)\n",
    "    return tf.keras.Model(inputs=input,outputs=x)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "630fddde-ef03-4a79-aa44-d2a11105a3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = get_text_vectorizer(for_vocabulary,vocab_size=30000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "319eb6da-798a-4a79-a268-34e3dee870c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vectorizer loaded\n",
      "Working on 32_0.01_True\n",
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         [(None, None)]            0         \n",
      "_________________________________________________________________\n",
      "text_vectorization (TextVect (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 500, 64)           1920000   \n",
      "_________________________________________________________________\n",
      "bi_gru_0 (Bidirectional)     (None, 500, 128)          49920     \n",
      "_________________________________________________________________\n",
      "attention_4 (Attention)      (None, 128)               628       \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 1,978,869\n",
      "Trainable params: 1,978,869\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/200\n",
      "547/547 [==============================] - 24s 37ms/step - loss: 0.7211 - binary_accuracy: 0.5007 - val_loss: 0.6931 - val_binary_accuracy: 0.4984\n",
      "Epoch 2/200\n",
      "547/547 [==============================] - 19s 34ms/step - loss: 0.6934 - binary_accuracy: 0.5007 - val_loss: 0.6932 - val_binary_accuracy: 0.4984\n",
      "Epoch 3/200\n",
      "547/547 [==============================] - 19s 34ms/step - loss: 0.6934 - binary_accuracy: 0.5007 - val_loss: 0.6934 - val_binary_accuracy: 0.4984\n",
      "Epoch 4/200\n",
      "547/547 [==============================] - 19s 34ms/step - loss: 0.6934 - binary_accuracy: 0.5007 - val_loss: 0.6932 - val_binary_accuracy: 0.4984\n",
      "Epoch 5/200\n",
      "547/547 [==============================] - 19s 34ms/step - loss: 0.6935 - binary_accuracy: 0.5007 - val_loss: 0.6932 - val_binary_accuracy: 0.4984\n",
      "Epoch 6/200\n",
      "547/547 [==============================] - 19s 34ms/step - loss: 0.6936 - binary_accuracy: 0.5007 - val_loss: 0.6932 - val_binary_accuracy: 0.4984\n",
      "Epoch 7/200\n",
      "547/547 [==============================] - 19s 34ms/step - loss: 0.6935 - binary_accuracy: 0.5007 - val_loss: 0.6932 - val_binary_accuracy: 0.4984\n",
      "Epoch 8/200\n",
      "547/547 [==============================] - 19s 34ms/step - loss: 0.6934 - binary_accuracy: 0.5007 - val_loss: 0.6931 - val_binary_accuracy: 0.4984\n",
      "Epoch 9/200\n",
      "547/547 [==============================] - 19s 34ms/step - loss: 0.6935 - binary_accuracy: 0.5007 - val_loss: 0.6932 - val_binary_accuracy: 0.4984\n",
      "Epoch 10/200\n",
      "547/547 [==============================] - 19s 34ms/step - loss: 0.6934 - binary_accuracy: 0.5007 - val_loss: 0.6933 - val_binary_accuracy: 0.4984\n",
      "Epoch 11/200\n",
      "547/547 [==============================] - 19s 34ms/step - loss: 0.6936 - binary_accuracy: 0.5007 - val_loss: 0.6932 - val_binary_accuracy: 0.4984\n",
      "For 32_0.01_True accuracy 0.5016 early_stopping 0.6931430101394653\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as gru_cell_13_layer_call_fn, gru_cell_13_layer_call_and_return_conditional_losses, gru_cell_14_layer_call_fn, gru_cell_14_layer_call_and_return_conditional_losses, gru_cell_13_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: results_gru/32_0.01_True/best_model.hd5/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: results_gru/32_0.01_True/best_model.hd5/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on 32_0.01_False\n",
      "Model: \"model_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         [(None, None)]            0         \n",
      "_________________________________________________________________\n",
      "text_vectorization (TextVect (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 500, 64)           1920000   \n",
      "_________________________________________________________________\n",
      "bi_gru_0 (Bidirectional)     (None, 500, 128)          49920     \n",
      "_________________________________________________________________\n",
      "attention_5 (Attention)      (None, 128)               628       \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 1,978,869\n",
      "Trainable params: 1,978,869\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/200\n",
      "547/547 [==============================] - 24s 37ms/step - loss: 0.3556 - binary_accuracy: 0.8362 - val_loss: 0.2465 - val_binary_accuracy: 0.8949\n",
      "Epoch 2/200\n",
      "547/547 [==============================] - 19s 34ms/step - loss: 0.1182 - binary_accuracy: 0.9548 - val_loss: 0.3303 - val_binary_accuracy: 0.8739\n",
      "Epoch 3/200\n",
      "547/547 [==============================] - 19s 34ms/step - loss: 0.0493 - binary_accuracy: 0.9820 - val_loss: 0.3915 - val_binary_accuracy: 0.8788\n",
      "Epoch 4/200\n",
      "547/547 [==============================] - 19s 34ms/step - loss: 0.0308 - binary_accuracy: 0.9894 - val_loss: 0.4419 - val_binary_accuracy: 0.8777\n",
      "Epoch 5/200\n",
      "241/547 [============>.................] - ETA: 8s - loss: 0.0298 - binary_accuracy: 0.9890"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 23\u001b[0m\n\u001b[1;32m     21\u001b[0m early_callback \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mEarlyStopping(patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m, restore_best_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     22\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdam(learning_rate\u001b[38;5;241m=\u001b[39mlearning_rate)\n\u001b[0;32m---> 23\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mearly_callback\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m evaluate_model(model, val_ds,result_path\u001b[38;5;241m=\u001b[39mresult_path)\n\u001b[1;32m     25\u001b[0m plot_training_graphs(history, result_path)\n",
      "Cell \u001b[0;32mIn[7], line 21\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_ds, val_ds, loss_fn, batch_size, optimizer, callbacks, epochs)\u001b[0m\n\u001b[1;32m     18\u001b[0m val_batched \u001b[38;5;241m=\u001b[39m val_ds\u001b[38;5;241m.\u001b[39mshuffle(\u001b[38;5;241m1000\u001b[39m)\u001b[38;5;241m.\u001b[39mbatch(batch_size)\n\u001b[1;32m     19\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(loss\u001b[38;5;241m=\u001b[39mloss_fn, optimizer\u001b[38;5;241m=\u001b[39moptimizer,\n\u001b[1;32m     20\u001b[0m           metrics\u001b[38;5;241m=\u001b[39m[tf\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mBinaryAccuracy(threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m)])\n\u001b[0;32m---> 21\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_batched\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_batched\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m history\n",
      "File \u001b[0;32m/common/cse479/shared/envs/tensorflow-env/lib/python3.9/site-packages/keras/engine/training.py:1184\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1177\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1178\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   1179\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   1180\u001b[0m     step_num\u001b[38;5;241m=\u001b[39mstep,\n\u001b[1;32m   1181\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   1182\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m   1183\u001b[0m   callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1184\u001b[0m   tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1185\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1186\u001b[0m     context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m/common/cse479/shared/envs/tensorflow-env/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py:885\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    882\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    884\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 885\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    887\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    888\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m/common/cse479/shared/envs/tensorflow-env/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py:917\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    914\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    915\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    916\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 917\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stateless_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    918\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    919\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    920\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    921\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m/common/cse479/shared/envs/tensorflow-env/lib/python3.9/site-packages/tensorflow/python/eager/function.py:3039\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3036\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m   3037\u001b[0m   (graph_function,\n\u001b[1;32m   3038\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m-> 3039\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3040\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/common/cse479/shared/envs/tensorflow-env/lib/python3.9/site-packages/tensorflow/python/eager/function.py:1963\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1959\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1960\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1961\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1962\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1963\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1964\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1965\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1966\u001b[0m     args,\n\u001b[1;32m   1967\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1968\u001b[0m     executing_eagerly)\n\u001b[1;32m   1969\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m/common/cse479/shared/envs/tensorflow-env/lib/python3.9/site-packages/tensorflow/python/eager/function.py:591\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    589\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    590\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 591\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    592\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    593\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    594\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    595\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    596\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    597\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    598\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    599\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[1;32m    600\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    603\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[1;32m    604\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m/common/cse479/shared/envs/tensorflow-env/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:59\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     58\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 59\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     62\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#experiment 1\n",
    "import pandas as pd\n",
    "from pathlib import Path \n",
    "loss_fn = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "# vectorizer = get_text_vectorizer(for_vocabulary)\n",
    "model_fn = get_bidirectional_gru_attention\n",
    "result_dir = Path(\"./results_gru\")\n",
    "exp_results = []\n",
    "print(\"vectorizer loaded\")\n",
    "for batch_size in [32,64]:\n",
    "    for learning_rate in [0.01,0.001]:\n",
    "        for regularizer in [True, False]:\n",
    "            s_result = {'batch_size':batch_size,'learning_rate':learning_rate,'regularizer':regularizer}\n",
    "            folder_name = f\"{batch_size}_{learning_rate}_{regularizer}\"\n",
    "            result_path = result_dir.joinpath(folder_name)\n",
    "            result_path.mkdir(parents=True,exist_ok=True)\n",
    "            print(\"Working on\",folder_name)\n",
    "            reg = tf.keras.regularizers.l2() if regularizer else None\n",
    "            model = model_fn(vectorizer=vectorizer,kernel_regularizer=reg,use_dropout=regularizer)\n",
    "            print(model.summary())\n",
    "            early_callback = tf.keras.callbacks.EarlyStopping(patience=10, monitor=\"val_loss\", restore_best_weights=True)\n",
    "            optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "            history = train_model(model, train_ds, val_ds, loss_fn, batch_size=batch_size, optimizer=optimizer, callbacks=[early_callback],epochs=200)\n",
    "            accuracy = evaluate_model(model, val_ds,result_path=result_path)\n",
    "            plot_training_graphs(history, result_path)\n",
    "            print(\"For\",folder_name,\"accuracy\",accuracy,\"early_stopping\", early_callback.best)\n",
    "            s_result['accuracy'] = accuracy\n",
    "            s_result['logs'] = history.history \n",
    "            model.save_weights(result_path.joinpath(\"best_weights.hd5\"))\n",
    "            model.save(result_path.joinpath(\"best_model.hd5\"))\n",
    "            # print(\"Accuracy on validation\", accuracy)\n",
    "            # break\n",
    "            s_result['model'] = result_path.joinpath(\"best_model.hd5\")\n",
    "            exp_results.append(s_result)\n",
    "            del model\n",
    "pd.DataFrame(exp_results).to_csv(\"./results_gru/exp_results.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0eeaf5-25b1-421f-8dea-515b148fe9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#experiment 1\n",
    "import pandas as pd\n",
    "from pathlib import Path \n",
    "loss_fn = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "# vectorizer = get_text_vectorizer(for_vocabulary)\n",
    "model_fn = get_bidirectional_lstm_attention\n",
    "result_dir = Path(\"./results_lstm\")\n",
    "exp_results = []\n",
    "print(\"vectorizer loaded\")\n",
    "for batch_size in [32,64]:\n",
    "    for learning_rate in [0.01,0.001]:\n",
    "        for regularizer in [True, False]:\n",
    "            s_result = {'batch_size':batch_size,'learning_rate':learning_rate,'regularizer':regularizer}\n",
    "            folder_name = f\"{batch_size}_{learning_rate}_{regularizer}\"\n",
    "            result_path = result_dir.joinpath(folder_name)\n",
    "            result_path.mkdir(parents=True,exist_ok=True)\n",
    "            print(\"Working on\",folder_name)\n",
    "            reg = tf.keras.regularizers.l2() if regularizer else None\n",
    "            model = model_fn(vectorizer=vectorizer,kernel_regularizer=reg,use_dropout=regularizer)\n",
    "            print(model.summary())\n",
    "            early_callback = tf.keras.callbacks.EarlyStopping(patience=10, monitor=\"val_loss\", restore_best_weights=True)\n",
    "            optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "            history = train_model(model, train_ds, val_ds, loss_fn, batch_size=batch_size, optimizer=optimizer, callbacks=[early_callback],epochs=200)\n",
    "            accuracy = evaluate_model(model, val_ds,result_path=result_path)\n",
    "            plot_training_graphs(history, result_path)\n",
    "            print(\"For\",folder_name,\"accuracy\",accuracy,\"early_stopping\", early_callback.best)\n",
    "            s_result['accuracy'] = accuracy\n",
    "            s_result['logs'] = history.history \n",
    "            model.save_weights(result_path.joinpath(\"best_weights.hd5\"))\n",
    "            model.save(result_path.joinpath(\"best_model.hd5\"))\n",
    "            # print(\"Accuracy on validation\", accuracy)\n",
    "            s_result['model'] = result_path.joinpath(\"best_model.hd5\")\n",
    "\n",
    "            # break\n",
    "            exp_results.append(s_result)\n",
    "            del model\n",
    "pd.DataFrame(exp_results).to_csv(\"./results_lstm/exp_results.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d60b9def-a3b9-44ef-96af-39bf7adec9da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1f31234e-2ce0-4777-978d-c9eb7dc5fbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "import main\n",
    "import model\n",
    "import util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3542392d-7ce8-4ea3-8e8c-ced6dd7bf6cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on 32_0.01_True\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m vocab \u001b[38;5;241m=\u001b[39m util\u001b[38;5;241m.\u001b[39mget_for_vocab_ds()\n\u001b[1;32m      5\u001b[0m vectorizer \u001b[38;5;241m=\u001b[39m util\u001b[38;5;241m.\u001b[39mget_text_vectorizer(vocab, vocab_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30000\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m results_l \u001b[38;5;241m=\u001b[39m \u001b[43mmain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_experiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvectorizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./results_lstm_\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m model_fn \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mget_bidirectional_gru_attention\n\u001b[1;32m      8\u001b[0m results_g \u001b[38;5;241m=\u001b[39m run_experiment(model_fn, train_ds, val_ds, loss_fn, vectorizer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./results_gru_\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/lustre/work/cse479/apoudel6/cse479_ap/hw3/main.py:19\u001b[0m, in \u001b[0;36mrun_experiment\u001b[0;34m(model_fn, train_ds, val_ds, loss_fn, vectorizer, logs)\u001b[0m\n\u001b[1;32m     17\u001b[0m result_path\u001b[38;5;241m.\u001b[39mmkdir(parents\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWorking on\u001b[39m\u001b[38;5;124m\"\u001b[39m,folder_name)\n\u001b[0;32m---> 19\u001b[0m reg \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mregularizers\u001b[38;5;241m.\u001b[39ml2() \u001b[38;5;28;01mif\u001b[39;00m regularizer \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     20\u001b[0m model \u001b[38;5;241m=\u001b[39m model_fn(vectorizer\u001b[38;5;241m=\u001b[39mvectorizer,kernel_regularizer\u001b[38;5;241m=\u001b[39mreg,use_dropout\u001b[38;5;241m=\u001b[39mregularizer)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(model\u001b[38;5;241m.\u001b[39msummary())\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "model_fn = model.get_bidirectional_lstm_attention\n",
    "train_ds, val_ds = util.get_train_val_ds()\n",
    "loss_fn = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "vocab = util.get_for_vocab_ds()\n",
    "vectorizer = util.get_text_vectorizer(vocab, vocab_size=30000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e5a590-64be-41d1-b39c-6d856298e8b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python CSCE479 (tensorflow-env)",
   "language": "python",
   "name": "tensorflow-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
